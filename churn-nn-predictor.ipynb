{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Churn with Neural Networks\n",
    "\n",
    "In this notebook, I attempt to build a model based on <code>Keras</code> library and train it on the same fictional telecoms company data used in other parts of this project. The code below is mainly adapted from <code>TensorFlow</code>'s [tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data) on how to handle imbalanced data.\n",
    "\n",
    "I will first train a model without taking the different class weights into account. I will then compare the results to another model that is trained with the class imbalance accounted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the necessary python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading various libraries needed for preprocessing, modelling and evaluating\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, recall_score, fbeta_score, \\\n",
    "                            precision_score, make_scorer, confusion_matrix, roc_curve, precision_recall_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset into a pandas dataframe\n",
    "customer_data = pd.read_csv('../data-sources/customer-churn/customer-churn.csv')\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the same naming convention to all column names\n",
    "customer_data.rename(columns={'customerID': 'CustomerID', 'gender': 'Gender', 'tenure': 'Tenure'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "#enoding the target variable into numerical binary\n",
    "customer_data['Churn'] = le.fit_transform(customer_data['Churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to convert numerical binary data into categorical binary data\n",
    "def ColumnTransformer(cell):\n",
    "    if cell == 0:\n",
    "        return 'No'\n",
    "    else:\n",
    "        return 'Yes'\n",
    "\n",
    "#applying the above function to the SeniorCitizen column\n",
    "customer_data['SeniorCitizen'] = customer_data['SeniorCitizen'].apply(ColumnTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to shorten the vocabulary for easier visualisation\n",
    "def ColumnTransformer(cell):\n",
    "    if cell == 'Electronic check':\n",
    "        return 'ElCh'\n",
    "    elif cell == 'Mailed check':\n",
    "        return 'MaCh'\n",
    "    elif cell == 'Bank transfer (automatic)':\n",
    "        return 'BaTr-A'\n",
    "    else:\n",
    "        return 'CrCa-A'\n",
    "    \n",
    "#applying the above function to the PaymentMethod column\n",
    "customer_data['PaymentMethod'] = customer_data['PaymentMethod'].apply(ColumnTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing all the customers who have been with the company for less than a month and deleting the CustomerID column\n",
    "customer_data.drop(customer_data[customer_data['Tenure'] == 0].index, inplace=True)\n",
    "customer_data.drop('CustomerID', axis=1, inplace=True)\n",
    "\n",
    "#converting TotalCharges from object to float data type\n",
    "customer_data['TotalCharges'] = customer_data['TotalCharges'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking another look at the imbalance present in the data\n",
    "print('Total: {}\\n    Positive: {:.2f}\\n    Negative: {:.2f}\\n'.format(customer_data.shape[0],\n",
    "                                                         customer_data['Churn'].value_counts(normalize=True)[1],\n",
    "                                                         customer_data['Churn'].value_counts(normalize=True)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##separating the target variable\n",
    "target = customer_data.pop('Churn')\n",
    "\n",
    "#dummifying the categorical features and removing any redundencies\n",
    "customer_data_dum = pd.get_dummies(customer_data, columns=customer_data.select_dtypes(include='object').columns,\n",
    "                                                   drop_first=True)\n",
    "\n",
    "#adding the target variable back to the dummyfied data\n",
    "customer_data_dum['Churn'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into different train, test and validation sets\n",
    "train_data, test_data = train_test_split(customer_data_dum, test_size=0.2, stratify=customer_data_dum['Churn'])\n",
    "train_data, validation_data = train_test_split(train_data, test_size=0.2, stratify=train_data['Churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating numpy arrays from the available labels and features\n",
    "train_labels = np.array(train_data.pop('Churn'))\n",
    "bool_train_labels = train_labels != 0\n",
    "validation_labels = np.array(validation_data.pop('Churn'))\n",
    "test_labels = np.array(test_data.pop('Churn'))\n",
    "\n",
    "train_features = np.array(train_data)\n",
    "validation_features = np.array(validation_data)\n",
    "test_features = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a StandardScaler object and standardising train, validation and test sets\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "validation_features = scaler.transform(validation_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', validation_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', validation_features.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the positive and negative churn instances into different dataframes\n",
    "pos_data = pd.DataFrame(train_features[bool_train_labels], columns=train_data.columns)\n",
    "neg_data = pd.DataFrame(train_features[~bool_train_labels], columns=train_data.columns)\n",
    "\n",
    "#surveying two sample distributions \n",
    "sns.jointplot(pos_data['MonthlyCharges'], pos_data['Tenure'], kind='hex')\n",
    "plt.suptitle('Positive distribution')\n",
    "\n",
    "sns.jointplot(neg_data['MonthlyCharges'], neg_data['Tenure'], kind='hex')\n",
    "plt.suptitle('Negative distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Model and Defining the Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlining the different metrics to use for model evaluation\n",
    "METRICS = [keras.metrics.TruePositives(name='tp'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.TrueNegatives(name='tn'),\n",
    "           keras.metrics.FalseNegatives(name='fn'),\n",
    "           keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "           keras.metrics.Precision(name='precision'),\n",
    "           keras.metrics.Recall(name='recall'),\n",
    "           keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.AUC(name='prc', curve='PR'),]\n",
    "\n",
    "#creating a function that creates a simple neural network with two densly connected hidden layers, two dropout layer\n",
    "#for regularization, and one output layer to return the probability of churn\n",
    "def MakeModel(metrics=METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = keras.Sequential([keras.layers.Dense(24, activation='relu', input_shape=(train_features.shape[-1],)),\n",
    "                              keras.layers.Dropout(0.5),\n",
    "                              keras.layers.Dense(16, activation='relu'),\n",
    "                              keras.layers.Dropout(0.4),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer = output_bias),\n",
    "                             ])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the number of epochs and the batch size used during modelling\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 150\n",
    "\n",
    "#defining the early stopping criteria\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_prc',\n",
    "                                                  verbose=1,\n",
    "                                                  patience=10,\n",
    "                                                  mode='max',\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "#building the model and printing out the summary\n",
    "model = MakeModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the model chart\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correcting the Initial Bias\n",
    "\n",
    "Given the imbalanced nature of the data, we know that the automatically generated initial bias might not be great. Here, I will first check the machine generated biases and the ncompare the Loss to a another model that reflects this imbalance in its output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making predictions for the first 10 observations to test the model\n",
    "model.predict(train_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the Loss for these few predictions\n",
    "results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "print('Loss: {:0.4f}'.format(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the initial bias based on the present imbalance\n",
    "neg, pos = np.bincount(customer_data_dum['Churn'])\n",
    "\n",
    "total = neg + pos\n",
    "\n",
    "initial_bias = np.log([pos/neg])\n",
    "initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserting the new bias figure into the model and making predictions\n",
    "model = MakeModel(output_bias=initial_bias)\n",
    "model.predict(train_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the Loss for these latest predictions\n",
    "results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "print('Loss: {:0.4f}'.format(results[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using the correct initial bias has reduced the Loss. Next, I will quantify how much this bias initialisation actually helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the initial weights in a checkpoint file for future use\n",
    "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
    "model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a model and setting the initial weights to zero \n",
    "model = MakeModel()\n",
    "model.load_weights(initial_weights)\n",
    "model.layers[-1].bias.assign([0.0])\n",
    "\n",
    "#fitting the model and checking against the validation set\n",
    "zero_bias_history = model.fit(train_features,\n",
    "                              train_labels,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              epochs=20,\n",
    "                              validation_data=(validation_features, validation_labels),\n",
    "                              verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a model with the calculated initial weights\n",
    "model = MakeModel()\n",
    "model.load_weights(initial_weights)\n",
    "\n",
    "#fitting the model and checking against the validation set\n",
    "careful_bias_history = model.fit(train_features,\n",
    "                                 train_labels,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 epochs=20,\n",
    "                                 validation_data=(validation_features, validation_labels),\n",
    "                                 verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to plot the loss metric over the range of epochs\n",
    "def PlotLoss(history, label, color):\n",
    "    plt.semilogy(history.epoch, history.history['loss'], color=color, label='Train ' + label)\n",
    "    plt.semilogy(history.epoch, history.history['val_loss'], color=color, label='Validation ' + label, ls='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the loss and validation loss for each weight bias\n",
    "plt.figure(figsize=(10, 6))\n",
    "PlotLoss(zero_bias_history, 'Zero Bias', 'black')\n",
    "PlotLoss(careful_bias_history, 'Careful Bias', 'crimson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the difference in this case isn't significant. However, using the correct initial weight is an important step in dealing with imbalanced data. Therefore, I will be using these initial weights from now on, regardless of their impact in this particular instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Weights\n",
    "\n",
    "In this section, I will first train and evaluate a model without passing class weights, followed by training and evaluating a model that takes class weights into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and validating a model with initial weights and predefined number of epochs \n",
    "model = MakeModel()\n",
    "model.load_weights(initial_weights)\n",
    "baseline_history = model.fit(train_features,\n",
    "                             train_labels,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             epochs=EPOCHS,\n",
    "                             callbacks=[early_stopping],\n",
    "                             validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to plot different model metrics against the number of epochs\n",
    "def PlotMetrics(history):\n",
    "    metrics = ['loss', 'prc', 'precision', 'recall']\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace('_', ' ').capitalize()\n",
    "        plt.subplot(2, 2, n+1)\n",
    "        plt.plot(history.epoch, history.history[metric], color='black', label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_' + metric], color='black', ls='--', label='Validation')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim([0, plt.ylim()[1]])\n",
    "        elif metric == 'auc':\n",
    "            plt.ylim([0.8, 1])\n",
    "        else:\n",
    "            plt.ylim([0, 1])\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the Loss, PRC, Precision, and Recall\n",
    "PlotMetrics(baseline_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making predictions on train and test sets\n",
    "train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to plot the confusion matricx\n",
    "def ConfMatrix(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the model metrics and plotting the confusion matrix\n",
    "baseline_results = model.evaluate(test_features, test_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "    print(name, ': ', value)\n",
    "\n",
    "ConfMatrix(test_labels, test_predictions_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to plot the ROC\n",
    "def PlotROC(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = roc_curve(labels, predictions)\n",
    "    plt.plot(100*fp, 100*tp, label=name, lw=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "    plt.xlim([-0.5, 100.5])\n",
    "    plt.ylim([-0.5, 100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the ROC for train and test predictions\n",
    "plt.figure(figsize=(8, 8))\n",
    "PlotROC('Train Baseline', train_labels, train_predictions_baseline, color='black')\n",
    "PlotROC('Test Baseline', test_labels, test_predictions_baseline, color='black', ls='--')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to plot the PRC\n",
    "def PlotPRC(name, labels, predictions, **kwargs):\n",
    "    precision, recall, _ = precision_recall_curve(labels, predictions)\n",
    "    plt.plot(precision, recall, label=name, lw=2, **kwargs)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the PRC for train and test predictions\n",
    "plt.figure(figsize=(8, 8))\n",
    "PlotPRC('Train Baseline', train_labels, train_predictions_baseline, color='black')\n",
    "PlotPRC('Test Baseline', test_labels, test_predictions_baseline, color='black', ls='--')\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calcualte the weight for each class by multiplying the inverse of class population by half of total population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating class weights\n",
    "weight_for_0 = (1/neg)*(total/2)\n",
    "weight_for_1 = (1/pos)*(total/2)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating and training a model using calculated initial weights and also class weights\n",
    "weighted_model = MakeModel()\n",
    "weighted_model.load_weights(initial_weights)\n",
    "weighted_history = weighted_model.fit(train_features,\n",
    "                                      train_labels,\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      epochs=EPOCHS,\n",
    "                                      callbacks=[early_stopping],\n",
    "                                      validation_data=(validation_features, validation_labels),\n",
    "                                      class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the evaluation metrics\n",
    "PlotMetrics(weighted_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making predictions on train and test sets\n",
    "train_predictions_weighted = weighted_model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the model metrics and plotting the confusion matrix\n",
    "weighted_results = weighted_model.evaluate(test_features, test_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "for name, value in zip(weighted_model.metrics_names, weighted_results):\n",
    "    print(name, ': ', value)\n",
    "\n",
    "ConfMatrix(test_labels, test_predictions_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the ROC for both models to compare the effect of explicitly supplying the class weigths\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "PlotROC('Train Baseline', train_labels, train_predictions_baseline, color='black')\n",
    "PlotROC('Test Baseline', test_labels, test_predictions_baseline, color='black', ls='--')\n",
    "\n",
    "PlotROC('Train Weighted', train_labels, train_predictions_weighted, color='crimson')\n",
    "PlotROC('Test weighted', test_labels, test_predictions_weighted, color='crimson', ls='--')\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the PRC for both models to compare the effect of explicitly supplying the class weigths\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "PlotPRC('Train Baseline', train_labels, train_predictions_baseline, color='black')\n",
    "PlotPRC('Test Baseline', test_labels, test_predictions_baseline, color='black', ls='--')\n",
    "\n",
    "PlotPRC('Train Weighted', train_labels, train_predictions_weighted, color='crimson')\n",
    "PlotPRC('Test Weighted', test_labels, test_predictions_weighted, color='crimson', ls='--')\n",
    "\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these plots, it seems that supplying class weights doesn't have a noticable effect on either the ROC or PRC. Perhaps the present imbalance is not large enough to be significant.\n",
    "\n",
    "Let's now look at one final metric, F1-beta score, as a way of comparing this neural network model with the more traditional models I trained in another [notebook](churn-predictor.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the probabilities to class predictions and calculating the F1-beta score\n",
    "predicted_labels = test_predictions_weighted > 0.5\n",
    "fbeta_score(test_labels, predicted_labels, beta=0.85, pos_label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the difference isn't large, this score compares favourably with the one achieved by the <code>VotingClassifier</code>. It is also important to note that this score can potentially be improved upon by careful optimisation, something that hasn't been considered in this project and can be the basis of future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeTuner(metrics=METRICS, output_bias=initial_bias, lr=1e-3, dr=0.5, top_neurons=5, bottom_neurons=5):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = keras.Sequential([keras.layers.Dense(top_neurons, activation='relu', input_shape=(train_features.shape[-1],)),\n",
    "                              keras.layers.Dropout(dr),\n",
    "                              keras.layers.Dense(bottom_neurons, activation='relu'),\n",
    "                              keras.layers.Dropout(dr),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer = output_bias),\n",
    "                             ])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss=keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_beta = make_scorer(fbeta_score, beta=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuner = KerasClassifier(build_fn=MakeTuner, verbose=0)\n",
    "\n",
    "param_grid = {'batch_size': [20, 100, 500],\n",
    "              'epochs': [50, 100, 150],\n",
    "              'lr': [1e-4, 1e-3],\n",
    "              'dr': [0.4, 0.5],\n",
    "              'top_neurons': [10, 25],\n",
    "              'bottom_neurons': [10, 25],\n",
    "              'class_weight': [class_weight]}\n",
    "\n",
    "model_tuner_gs = GridSearchCV(model_tuner, param_grid, cv=3, scoring=f1_beta)\n",
    "tuning_result = model_tuner_gs.fit(train_features, train_labels)\n",
    "\n",
    "print(\"Cross-validated F1-beta score: %f using %s\" % (tuning_result.best_score_, tuning_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_tuned = model_tuner_gs.predict(train_features)\n",
    "test_predictions_tuned = model_tuner_gs.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfMatrix(test_labels, test_predictions_tuned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
